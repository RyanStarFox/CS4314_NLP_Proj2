import streamlit as st
import time
import json
from question_db import QuestionDB

st.set_page_config(page_title="é”™é¢˜æ•´ç†", page_icon="logo.webp", layout="wide")

st.markdown("""
<style>
    .stButton button {
        width: 100%;
        border-radius: 8px;
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ““ é”™é¢˜æ•´ç†")

question_db = QuestionDB()
wrong_questions = question_db.get_wrong_questions()

if not wrong_questions:
    st.info("ğŸ‰ å¤ªæ£’äº†ï¼ç›®å‰é”™é¢˜æœ¬æ˜¯ç©ºçš„ã€‚å¿«å»ã€åšé¢˜ç»ƒä¹ ã€‘å§ï¼")
    if st.button("å‰å¾€åšé¢˜ç»ƒä¹ "):
        st.switch_page("pages/2_ğŸ“_åšé¢˜ç»ƒä¹ .py")
    st.stop()

# Session State for Re-quiz
if "mistake_index" not in st.session_state:
    st.session_state.mistake_index = 0
if "mistake_mode" not in st.session_state:
    st.session_state.mistake_mode = "list" # list, quiz

# --- Mode: List View ---
if st.session_state.mistake_mode == "list":
    st.markdown(f"### å…± {len(wrong_questions)} é“é”™é¢˜")
    
    col_act1, col_act2 = st.columns([1, 1])
    with col_act1:
        if st.button("ğŸ“ å¼€å§‹å¤ä¹ æ¨¡å¼ (é€ä¸ªé‡åš)", type="primary", use_container_width=True):
            st.session_state.mistake_mode = "quiz"
            st.session_state.mistake_index = 0
            st.rerun()
    with col_act2:
        expand_all = st.checkbox("ğŸ“– å±•å¼€æ‰€æœ‰é¢˜ç›®", value=False)

    # Manual Question Upload
    with st.expander("â• æ‰‹åŠ¨æ·»åŠ é”™é¢˜", expanded=False):
        with st.form("manual_add_mistake"):
            st.info("ğŸ’¡ æç¤ºï¼šä¸Šä¼ é¢˜ç›®å›¾ç‰‡åï¼Œç³»ç»Ÿå°†å°è¯•è‡ªåŠ¨è¯†åˆ«é¢˜ç›®å†…å®¹å’Œé€‰é¡¹ã€‚")
            uploaded_q_image = st.file_uploader("ä¸Šä¼ é¢˜ç›®å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰", type=["jpg", "png", "jpeg"])
            
            # Use columns for text inputs to save space if needed, or just standard
            q_content = st.text_area("é¢˜ç›®å†…å®¹ (ç•™ç©ºåˆ™å°è¯•ä»å›¾ç‰‡è‡ªåŠ¨æå–)", placeholder="è¯·è¾“å…¥é¢˜ç›®æ–‡æœ¬...", height=100)
            q_options = st.text_area("é€‰é¡¹ (æ¯è¡Œä¸€ä¸ªï¼Œç•™ç©ºåˆ™å°è¯•è‡ªåŠ¨æå–)", placeholder="A. é€‰é¡¹1\nB. é€‰é¡¹2\n...", height=100)
            
            col1, col2 = st.columns(2)
            with col1:
                q_correct = st.text_input("æ­£ç¡®ç­”æ¡ˆ (å¯é€‰)", placeholder="ä¾‹å¦‚ï¼šA")
            with col2:
                q_explanation = st.text_area("è§£æ (ç•™ç©ºåˆ™ç”± AI ç”Ÿæˆ)", placeholder="è¯·è¾“å…¥è§£æ...", height=100)
            
            submitted = st.form_submit_button("æ™ºèƒ½è¯†åˆ«å¹¶æ·»åŠ ")
            
            if submitted:
                # è‡³å°‘éœ€è¦æœ‰å›¾ç‰‡ æˆ–è€… æœ‰é¢˜ç›®å†…å®¹
                # å¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œå¿…é¡»æœ‰é¢˜ç›®å†…å®¹
                # å¦‚æœæœ‰å›¾ç‰‡ï¼Œé¢˜ç›®å†…å®¹å¯ä»¥ä¸ºç©º
                
                valid_input = False
                if uploaded_q_image:
                    valid_input = True
                elif q_content:
                    valid_input = True
                    
                if not valid_input:
                    st.error("è¯·è‡³å°‘è¾“å…¥é¢˜ç›®æ–‡æœ¬æˆ–ä¸Šä¼ å›¾ç‰‡")
                else:
                    with st.spinner("æ­£åœ¨å¤„ç†..."):
                        import base64
                        from openai import OpenAI
                        from config import OPENAI_API_KEY, OPENAI_API_BASE, VL_MODEL_NAME
                        
                        client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)
                        
                        final_question = q_content
                        final_options = q_options
                        final_correct = q_correct
                        final_explanation = q_explanation
                        
                        # 1. Image Processing (Extraction)
                        if uploaded_q_image and (not q_content or not q_options):
                            try:
                                img_b64 = base64.b64encode(uploaded_q_image.getvalue()).decode('utf-8')
                                extract_prompt = """è¯·è¯†åˆ«è¿™å¼ å›¾ç‰‡ä¸­çš„é¢˜ç›®ã€‚
                                è¯·ä»¥ä¸¥æ ¼çš„ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å« Markdown ä»£ç å—ã€‚
                                æ ¼å¼å¦‚ä¸‹ï¼š
                                {
                                    "question": "é¢˜ç›®æ–‡æœ¬",
                                    "options": ["é€‰é¡¹Aå†…å®¹", "é€‰é¡¹Bå†…å®¹", ...],
                                    "correct_answer": "å¦‚æœæœ‰æ ‡å‡†ç­”æ¡ˆè¯·å¡«åœ¨è¿™é‡Œï¼Œå¦åˆ™ç•™ç©º",
                                    "explanation": "å¦‚æœæœ‰è§£æè¯·å¡«åœ¨è¿™é‡Œï¼Œå¦åˆ™ç•™ç©º"
                                }
                                """
                                response = client.chat.completions.create(
                                    model=VL_MODEL_NAME,
                                    messages=[
                                        {
                                            "role": "user", 
                                            "content": [
                                                {"type": "text", "text": extract_prompt},
                                                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_b64}"}}
                                            ]
                                        }
                                    ],
                                    response_format={"type": "json_object"}
                                )
                                extracted = json.loads(response.choices[0].message.content)
                                
                                if not final_question: final_question = extracted.get("question", "")
                                if not final_options: final_options = "\n".join(extracted.get("options", []))
                                if not final_correct: final_correct = extracted.get("correct_answer", "")
                                if not final_explanation: final_explanation = extracted.get("explanation", "")
                                
                            except Exception as e:
                                st.warning(f"å›¾ç‰‡è¯†åˆ«å¤±è´¥: {e}")
                        
                        # 2. Answer & Explanation Generation (if missing)
                        if not final_correct or not final_explanation:
                            try:
                                solve_prompt = f"""
                                é¢˜ç›®ï¼š{final_question}
                                é€‰é¡¹ï¼š{final_options}
                                
                                è¯·åšè¿™é“é¢˜ã€‚
                                1. ç»™å‡ºæ­£ç¡®é€‰é¡¹ï¼ˆä¾‹å¦‚ "A" æˆ– "é€‰é¡¹å†…å®¹"ï¼‰ã€‚
                                2. ç»™å‡ºè¯¦ç»†è§£æã€‚
                                
                                è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼š
                                {{
                                    "correct_answer": "...",
                                    "explanation": "..."
                                }}
                                """
                                # Use standard model for solving if text is available
                                from config import MODEL_NAME
                                solve_resp = client.chat.completions.create(
                                    model=MODEL_NAME,
                                    messages=[{"role": "user", "content": solve_prompt}],
                                    response_format={"type": "json_object"}
                                )
                                solution = json.loads(solve_resp.choices[0].message.content)
                                
                                if not final_correct: final_correct = solution.get("correct_answer", "")
                                # User requested "Always generate explanation" (LLMè‡ªå·±ç”Ÿæˆè§£æ)
                                # So we prefer LLM explanation unless user provided one?
                                # User said: "å§‹ç»ˆè‡ªå·±ç”Ÿæˆè§£æ" -> Assuming if user left it blank, generate. 
                                # But actually "å§‹ç»ˆ" implies overwrite? Let's stick to "if blank" for better UX, or append.
                                # Let's overwrite if blank.
                                if not final_explanation: final_explanation = solution.get("explanation", "")
                                
                            except Exception as e:
                                print(f"è§£æç”Ÿæˆå¤±è´¥: {e}")

                        # Construct Final Data
                        options_list = [opt.strip() for opt in final_options.split('\n') if opt.strip()]
                        if not options_list: options_list = ["(æœªè¯†åˆ«åˆ°é€‰é¡¹)"]
                        
                        question_data = {
                            "question": final_question if final_question else "ï¼ˆæœªè¯†åˆ«é¢˜ç›®ï¼‰",
                            "options": options_list,
                            "correct_answer": final_correct if final_correct else "ï¼ˆæœªçŸ¥ï¼‰",
                            "explanation": final_explanation if final_explanation else "æš‚æ— è§£æ"
                        }
                        
                        # Generate Summary for Manual Question
                        summary = None
                        try:
                            # Use existing logic to generate summary
                            sum_prompt = f"è¯·ç”¨ä¸è¶…è¿‡20ä¸ªå­—æ€»ç»“ä»¥ä¸‹é¢˜ç›®çš„æ ¸å¿ƒè€ƒç‚¹æˆ–é—®é¢˜å¤§æ„ï¼š\n{final_question}"
                            
                            # Use standard model for summarization
                            from config import MODEL_NAME
                            sum_resp = client.chat.completions.create(
                                model=MODEL_NAME,
                                messages=[{"role": "user", "content": sum_prompt}],
                                max_tokens=50,
                                temperature=0.3
                            )
                            summary = sum_resp.choices[0].message.content.strip()
                        except Exception as e:
                            print(f"Summary generation failed: {e}")
                            summary = final_question[:20] + "..." if final_question else "å›¾ç‰‡é¢˜ç›®"

                        question_db.add_result(
                            kb_name="Manual_Upload", 
                            question_data=question_data,
                            user_answer="ï¼ˆæ‰‹åŠ¨æ·»åŠ ï¼‰",
                            is_correct=False,
                            summary=summary
                        )
                        st.success("æ·»åŠ æˆåŠŸï¼")
                        time.sleep(1)
                        st.rerun()

    st.markdown("---")

    for i, item in enumerate(wrong_questions):
        q = item["question"]
        question_text = q.get('question')
        
        # Summary logic: Use LLM summary if available, else truncate
        summary = item.get("summary")
        if not summary:
            summary = question_text[:20] + "..." if len(question_text) > 20 else question_text
        
        with st.expander(f"âŒ é”™é¢˜ {i+1}: {summary}", expanded=expand_all):
            st.markdown(f"**é¢˜ç›®ï¼š** {question_text}")
            st.markdown("**é€‰é¡¹ï¼š**")
            options = q.get("options", [])
            for opt in options:
                st.text(f"- {opt}")
            
            st.markdown(f"**ä½ çš„é”™è¯¯ç­”æ¡ˆï¼š** âŒ {item.get('user_answer')}")
            
            # Editable Correct Answer
            current_correct = q.get('correct_answer')
            col_ans, col_edit = st.columns([3, 1])
            with col_ans:
                st.markdown(f"**æ­£ç¡®ç­”æ¡ˆï¼š** âœ… {current_correct}")
            with col_edit:
                with st.popover("âœï¸ ä¿®æ”¹ç­”æ¡ˆ"):
                    new_correct = st.selectbox("ä¿®æ­£æ­£ç¡®ç­”æ¡ˆä¸º:", options, index=options.index(current_correct) if current_correct in options else 0, key=f"edit_ans_{item['id']}")
                    if st.button("ç¡®è®¤ä¿®æ”¹", key=f"confirm_edit_{item['id']}"):
                        question_db.update_correct_answer(item['id'], new_correct)
                        st.rerun()

            st.info(f"ğŸ’¡ **è§£æï¼š** {q.get('explanation')}")
            
            if st.button("ğŸ—‘ï¸ æˆ‘å·²æŒæ¡ï¼Œç§»å‡ºé”™é¢˜æœ¬", key=f"del_{item['id']}"):
                question_db.remove_wrong_question(item['id'])
                st.rerun()

# --- Mode: Quiz View ---
elif st.session_state.mistake_mode == "quiz":
    # Reload in case some were deleted
    wrong_questions = question_db.get_wrong_questions()
    if not wrong_questions:
        st.session_state.mistake_mode = "list"
        st.rerun()
        
    idx = st.session_state.mistake_index
    if idx >= len(wrong_questions):
        st.success("ğŸ‰ å¤ä¹ å®Œæˆï¼")
        if st.button("è¿”å›åˆ—è¡¨"):
            st.session_state.mistake_mode = "list"
            st.rerun()
        st.stop()
        
    item = wrong_questions[idx]
    q = item["question"]
    
    st.progress((idx + 1) / len(wrong_questions))
    st.caption(f"é”™é¢˜å¤ä¹  {idx + 1} / {len(wrong_questions)}")
    
    st.markdown(f"### {q.get('question')}")
    
    # State for current question feedback
    if f"mistake_answered_{item['id']}" not in st.session_state:
        st.session_state[f"mistake_answered_{item['id']}"] = False
        
    options = q.get("options", [])
    correct_option = q.get("correct_answer")
    
    answered = st.session_state[f"mistake_answered_{item['id']}"]
    
    if not answered:
        for opt in options:
            if st.button(opt, key=f"mq_{item['id']}_{opt}", use_container_width=True):
                if opt == correct_option:
                    st.toast("âœ… å›ç­”æ­£ç¡®ï¼")
                    st.session_state[f"mistake_answered_{item['id']}"] = True
                    st.rerun()
                else:
                    st.toast("âŒ ä¾ç„¶é”™è¯¯ï¼Œè¯·å†æƒ³æƒ³", icon="âŒ")
    else:
        st.success(f"âœ… æ­£ç¡®ç­”æ¡ˆï¼š{correct_option}")
        st.info(f"ğŸ’¡ è§£æï¼š{q.get('explanation')}")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ—‘ï¸ æ ‡è®°ä¸ºå·²æŒæ¡ (ç§»å‡º)", key=f"mq_del_{item['id']}", type="primary"):
                question_db.remove_wrong_question(item['id'])
                # Adjust index if needed? If we delete, the next item slides into this index.
                # So we don't increment index, but we need to reset the state for the new item at this index?
                # Actually, easier to just increment index for flow, or reload.
                # If we delete, len decreases. 
                # Let's just remove and stay at same index (which is now next item).
                # But we need to clear session state for the 'next' item ID if it was recycled?
                # Using ID in key helps.
                st.rerun()
        with col2:
            if st.button("â¡ï¸ ä¸‹ä¸€é¢˜", key=f"mq_next_{item['id']}"):
                st.session_state.mistake_index += 1
                st.rerun()

    if st.button("ğŸ”™ é€€å‡ºå¤ä¹ ", type="secondary"):
        st.session_state.mistake_mode = "list"
        st.rerun()

